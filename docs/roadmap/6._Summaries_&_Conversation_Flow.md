_For the high level project overview, go to: [HIGH-Level_Roadmap_(Tangent)](HIGH-Level_Roadmap_(Tangent))

Below is the **low-level roadmap for Step 6: Summaries & Conversation Flow**, which will guide you through generating short and long summaries in tandem with the LLM’s main response, and storing everything in the database so that conversation data can be recalled and expanded upon. This sets the foundation for branching logic in later steps.

---

## **6. Summaries & Conversation Flow**✔️

### **6.1 Expand the LLM Logic to Include Summaries**✔️

Since we decided on a **single call** approach, we’ll prompt the LLM once, asking it for:

1. The **main answer** to the user’s question,
2. A **short summary** (~40–80 chars),
3. A **long summary** (~350 chars).

We’ll store these three pieces in a single response. Here’s the broad idea:

1. **Craft a Prompt** within `llm.js` (or a new function) that instructs the LLM to output all three parts.
2. **Parse** the response carefully to separate the main answer from the short and long summaries.

---

#### 6.1.1 Adjust `llm.js` (or create `summaries.js`) ✔️

1. **Create a dedicated function** to handle the “multi-output” logic. For example, in `llm.js`:
``` js
// src/server/controllers/llm.js
const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

// We’ll build a single function that returns all 3 items:
async function getLLMResponseWithSummaries(userInput) {
  try {
    // Build a prompt that requests the main answer, plus short & long summaries
    const systemInstructions = `
      You are an AI assistant. You will receive a user input.
      1) Provide a response ("mainAnswer").
      2) Then provide a short summary of your answer (40–80 chars) labeled "summaryShort".
      3) Then provide a longer summary (~350 chars) labeled "summaryLong".
      IMPORTANT: Format your response in JSON only, e.g.:
      {
        "mainAnswer": "...",
        "summaryShort": "...",
        "summaryLong": "..."
      }
      Do NOT include any additional keys or text besides that JSON.
    `;

    const response = await openai.createChatCompletion({
      model: 'gpt-3.5-turbo',
      messages: [
        { role: 'system', content: systemInstructions },
        { role: 'user', content: userInput }
      ],
      temperature: 0.7, // adjust as desired
    });

    const rawText = response.data.choices[0].message.content.trim();

    // The AI should return valid JSON. Let's parse it:
    let parsed;
    try {
      parsed = JSON.parse(rawText);
    } catch (err) {
      console.error('Failed to parse LLM JSON:', rawText);
      throw new Error('LLM did not return valid JSON');
    }

    // Check if keys exist
    const {
      mainAnswer = "No main answer found",
      summaryShort = "No short summary",
      summaryLong = "No long summary"
    } = parsed;

    return { mainAnswer, summaryShort, summaryLong };
  } catch (error) {
    console.error("Error calling OpenAI API:", error);
    throw error;
  }
}

module.exports = { getLLMResponseWithSummaries };
```
1. **Explanation**
    
    - We add a **system prompt** that instructs the LLM to return structured JSON containing three properties.
    - We parse the JSON to separate out `mainAnswer`, `summaryShort`, and `summaryLong`.
    - If the AI doesn’t follow instructions perfectly, we attempt a fallback or throw an error.

---

### **6.2 Update the `/api/chat` Route to Use the Summaries**✔️

1. **Modify `index.js`**:
``` js
// src/server/index.js
const { getLLMResponseWithSummaries } = require('./controllers/llm');
const prisma = require('./db'); // If you haven't already, import your Prisma client

app.post('/api/chat', async (req, res) => {
  const userInput = req.body.message;

  // For now, let’s assume we have a single conversation ID (e.g., 1). 
  // Later, we’ll pass conversationId or handle it more dynamically.
  const conversationId = 1;

  try {
    // 1) Get the main answer + short and long summaries
    const { mainAnswer, summaryShort, summaryLong } = await getLLMResponseWithSummaries(userInput);

    // 2) Save to DB as a "node" in your conversation
    //    We'll treat this newly created node as a MAIN branch or user prompt/response pair
    //    but it's up to you how you handle user prompts vs. AI responses in DB.
    //    For example, if you treat each user prompt and each AI response as separate nodes,
    //    you'd do that differently. Here we keep it simple.
    const newNode = await prisma.node.create({
      data: {
        content: `${userInput}\n\n${mainAnswer}`, // or store them separately if you prefer
        summaryShort,
        summaryLong,
        branchType: 'MAIN',
        conversationId
      }
    });

    // 3) Return the main answer to the front-end
    res.json({ 
      response: mainAnswer, 
      shortSummary: summaryShort, 
      longSummary: summaryLong,
      nodeId: newNode.id
    });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Failed to process your request.' });
  }
});
```
1. **Check the DB**
    
    - After you call this endpoint, you should see a new row in `Node` with:
        - `content`: containing both the user’s question and the LLM’s answer.
        - `summaryShort` + `summaryLong` stored.
    - In a more sophisticated architecture, you might store user prompts and LLM responses as **separate** nodes. That’s up to your design.

2. **Test It**
    
    - Send a POST request to `/api/chat` with JSON `{"message": "Tell me about quantum physics"}`.
    - Check the response. It should have keys `response`, `shortSummary`, `longSummary`, etc.
    - Check your DB (via Prisma Studio or `SELECT * FROM Node`) to see the data inserted.

---

### **6.3 Conversation Flow**✔️

Now that you can store a node for each request-response cycle, you can begin building:

1. **Multiple Nodes Per Conversation**
	
    - The code snippet above always uses `conversationId = 1` for simplicity. Eventually, you’ll track which conversation the user is in or create a new conversation record if one doesn’t exist.
    - **Flow**: When the user sends a prompt, you create a “prompt” node and/or an “answer” node. Or store them together in a single node.

2. **Branching**
    
    - The next step (often Step 7 or 8 in our high-level plan) is to let the user “fork” from a previous node. That’s where you set `parentId` to that node’s ID.
    - The short and long summaries let you quickly identify and display each node in your “overview.”

3. **Displaying the Flow**
	
    - On the front-end, you’ll fetch a conversation’s nodes (`GET /api/conversation/:id`), then render them in a vertical chat or a 2D tree.
    - Each node has the text content plus the short summary, which you can show as a tooltip or label.


---

### **6.4 Testing & Validation**✔️

1. **Try Various Prompts**
    
    - Confirm short summary length is near 40–80 chars and the long summary is near 350 (the LLM might produce fewer or more characters, so you might have to tweak your prompt or do some post-processing).

2. **Check DB**
    
    - Use `npx prisma studio` or a SQL tool to see each new node.

3. **Handle Edge Cases**
    
    - If the LLM fails to return valid JSON, you’ll get an error. You might want to catch that and attempt to re-call the LLM or default to some fallback logic.


---

### **6.5 Commit Your Changes**✔️

When it’s working:

``` bash
git add src/server/controllers/llm.js src/server/index.js 
git commit -m "Added single-call LLM response with short/long summaries and DB storage"
```

---

## **Summary of Step 6** ✔️

- **Objective**: Generate short and long summaries in a **single LLM call** and store them in the database (along with the main answer).

- **Implementation**:
    
    1. Enhanced your LLM call to request JSON containing `mainAnswer`, `summaryShort`, `summaryLong`.
    2. Stored those fields in `Node` via Prisma.
    3. Returned the main answer (and optional summaries) back to the client.

- **Outcome**: Now you can keep track of conversation nodes, each with quick-scan summaries for your future branching or overview features.


---

### **Moving Forward**

After you finish Step 6, the next steps typically involve:

1. **Refining the Conversation Data Model** (deciding how you split user prompts and AI responses).
2. **Building/Enhancing the GET endpoints** to load a full conversation by ID (plus all nodes).
3. **Branching Logic** (tangents, `parentId` references).
4. **Creating a front-end** (Vue) that displays the conversation in a user-friendly way.

Let me know if you have any questions. Otherwise, once Step 6 is done, we can move on to the next part of your roadmap!