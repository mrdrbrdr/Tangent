_For the high level project overview, go to: [HIGH-Level_Roadmap_(Tangent)](HIGH-Level_Roadmap_(Tangent).md)

Below is a detailed low-level roadmap for **Step 5: LLM Integration (OpenAI)**. In this step, you'll integrate the OpenAI API into your server so that your `/api/chat` route fetches an LLM response based on user input. We'll create a dedicated utility function for calling OpenAI and then hook it into your Express route.

---

## **5. LLM Integration (OpenAI)**

### **5.1 Secure Your API Key**

1. **Open your `.env` file** in the project root (if you haven't already) and add your OpenAI API key:
    `OPENAI_API_KEY=your-openai-api-key`

2. Ensure your `.env` file is in `.gitignore` so your key isn’t committed.

---

### **5.2 Verify the OpenAI Package Installation**

- Confirm that the OpenAI client library is installed. If not, install it:
    `npm install openai`


---

### **5.3 Create an LLM Controller**

1. **Create a new directory (if needed):**  
    Within your project structure, create a new folder for controllers if it doesn’t exist:
    `mkdir -p src/server/controllers`

2. **Create the File `llm.js`** inside `src/server/controllers` with the following content:
```
// src/server/controllers/llm.js
const { Configuration, OpenAIApi } = require('openai');

// Initialize OpenAI configuration with your API key from the environment variables.
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

// Function to get a response from the LLM
async function getLLMResponse(userInput) {
  try {
    const response = await openai.createChatCompletion({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: userInput }],
      // You can adjust parameters like temperature, max_tokens, etc.
    });
    return response.data.choices[0].message.content;
  } catch (error) {
    console.error("Error calling OpenAI API:", error);
    throw error;
  }
}

module.exports = { getLLMResponse };

```

    **Explanation:**
    - We initialize the OpenAI API with your secure API key.
    - The `getLLMResponse` function sends a request using the `gpt-3.5-turbo` model, passing the user’s message, and returns the response text.

---

### **5.4 Integrate LLM Functionality in Your Express Route**

1. **Modify `src/server/index.js`:**
    - Import the `getLLMResponse` function at the top of your file:
        `const { getLLMResponse } = require('./controllers/llm');`
    - Update your `/api/chat` route to call this function:
``` 
// POST /api/chat route
app.post('/api/chat', async (req, res) => {
  const userInput = req.body.message;
  try {
    const llmResponse = await getLLMResponse(userInput);
    res.json({ response: llmResponse });
  } catch (error) {
    res.status(500).json({ error: "Failed to get response from LLM" });
  }
});
```

1. **Save your changes** and restart your server using:
    `npm run dev`

---

### **5.5 Test the Integration**

1. **Using Insomnia or Postman:**
    - Send a POST request to `http://localhost:3000/api/chat` with a JSON body such as:
```
{
  "message": "Hello, how are you?"
}
```

2. **Expected Outcome:**
    - You should receive a JSON response that includes the LLM-generated text, for example:
```
{
  "response": "I'm doing great, thank you! How can I assist you today?"
}
```

3. **Troubleshoot:**
    - Check your console for any error logs if something isn’t working.


---

### **5.6 Commit Your Changes**

Once everything is working:

1. Stage your changes:
    `git add src/server/controllers/llm.js src/server/index.js .env`

2. Commit with a meaningful message:
    `git commit -m "Integrate OpenAI API for LLM response in /api/chat route"`

---

## **Next Steps**

With LLM integration complete, the `/api/chat` endpoint now dynamically fetches responses from the OpenAI API. Next, we can expand the functionality to store conversation data in the database, integrate summaries, and gradually build out the remaining features of your Tangent project.